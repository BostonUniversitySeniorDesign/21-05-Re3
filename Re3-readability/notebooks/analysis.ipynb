{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier,AdaBoostRegressor, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.inspection import permutation_importance\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching and preparing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = pd.read_csv('../data/features.csv', index_col= 0)\n",
    "features = pd.read_csv('../data/features_with_vars.csv', index_col=0)\n",
    "\n",
    "# Uncomment desired rating mode\n",
    "# ratings = pd.read_csv('../data/average_ratings.csv',index_col = 0)\n",
    "ratings = pd.read_csv('../data/average_ratings_no_outliers.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the line below if you are using features.csv instead of features_with_vars.csv\n",
    "# features_reduced  = features.iloc[:,2:-6]\n",
    "# comment the line below if you are using features.csv instead of features_with_vars.csv\n",
    "features_reduced = features.iloc[:,2:-1] \n",
    "\n",
    "X = features_reduced.values\n",
    "\n",
    "y = ratings.iloc[:,1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "\n",
    "\n",
    "array_of_accuracies = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training model\n",
    "linear_regression_model = LinearRegression()\n",
    "reg = linear_regression_model.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38816686631751546\n"
     ]
    }
   ],
   "source": [
    "# Evaluating model\n",
    "reg_y_pred = reg.predict(X_test)\n",
    "acc_mse = mean_squared_error(reg_y_pred,y_test)\n",
    "print(mean_squared_error(reg_y_pred,y_test))\n",
    "\n",
    "array_of_accuracies[\"Linear Regression\"] = acc_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'linear_model.sav'\n",
    "pickle.dump(reg, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training svm model with rbf kernel\n",
    "SVM = svm.SVR()\n",
    "\n",
    "# SVM HyperParameter Tuning\n",
    "K = 10\n",
    "\n",
    "# specifying list of parameters to be searched over\n",
    "max_iter_int = -1\n",
    "kernel = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "degree = [2,4,6,8,10]\n",
    "tolfloat = 1e-5\n",
    "\n",
    "# creating variables that will keep track of best score and parameters\n",
    "best_score = 10\n",
    "hp = {\"kernel\" : kernel[0], \"degree\" : 0}\n",
    "counter = 1\n",
    "\n",
    "# looping over said parameters\n",
    "for j in kernel:\n",
    "    count = 0\n",
    "    for k in degree:    \n",
    "        # creating rf with the parameters and then getting the cross val score for it\n",
    "        if(j == \"poly\" or count == 0):\n",
    "            svm_reg = svm.SVR(kernel = j, degree = k)\n",
    "            score = cross_val_score(svm_reg,X_train,y_train,cv = K, scoring = \"neg_mean_squared_error\")\n",
    "            # storing the param if they show improvements\n",
    "            if(abs(np.mean(score)) < best_score):\n",
    "                best_score = abs(np.mean(score))\n",
    "                hp = {\"kernel\" : j, \"degree\": k}\n",
    "\n",
    "            counter+=1    \n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6526038867511781\n"
     ]
    }
   ],
   "source": [
    "# Assigning the best hyperparameters to hp_svm_reg\n",
    "hp_svm_reg = hp\n",
    "\n",
    "# creating the svm_reg based on those parameters\n",
    "svm_reg = svm.SVR(kernel = hp_svm_reg[\"kernel\"], degree = hp_svm_reg[\"degree\"])\n",
    "svm_reg.fit(X_train,y_train)\n",
    "\n",
    "# using svm_reg to evaluate the test set\n",
    "svm_y_pred = svm_reg.predict(X_test)\n",
    "\n",
    "# getting the accuracy of the prediction\n",
    "acc_mse = mean_squared_error(svm_y_pred,y_test)\n",
    "print(acc_mse)\n",
    "\n",
    "array_of_accuracies[\"SVM\"] = (acc_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "\n",
    "# specifying list of parameters to be searched over\n",
    "max_d = [1,2,3,4,5]\n",
    "max_f = [\"sqrt\",\"log2\",\"auto\"]\n",
    "estimators = np.linspace(10,400,5)\n",
    "\n",
    "# creating variables that will keep track of best score and parameters\n",
    "best_score = 10\n",
    "hp = {\"max_depth\" : max_d[0], \"max_features\": max_f[0], \"n_estimators\": int(estimators[0])}\n",
    "counter = 1\n",
    "\n",
    "# looping over said parameters\n",
    "for j in max_d:\n",
    "    for k in max_f:\n",
    "        for l in estimators:\n",
    "            \n",
    "            # creating rf with the parameters and then getting the cross val score for it\n",
    "            rf = RandomForestRegressor(random_state=123, n_estimators = int(l),max_features = k,max_depth = j)\n",
    "            score = cross_val_score(rf,X_train,y_train,cv = K, scoring = \"neg_mean_squared_error\")\n",
    "            \n",
    "            # storing the param if they show improvements\n",
    "            if(abs(np.mean(score)) < best_score):\n",
    "                best_score = abs(np.mean(score))\n",
    "                hp = {\"max_depth\" : j, \"max_features\": k, \"n_estimators\": int(l)}\n",
    "#             print(abs((np.mean(score))))\n",
    "#             print(counter)\n",
    "            counter+=1                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4552730279559924\n"
     ]
    }
   ],
   "source": [
    "# print the best params found from the previous snippet\n",
    "hp_rf = hp\n",
    "\n",
    "# Evaluating model\n",
    "rf = RandomForestRegressor(random_state=123,n_estimators = hp_rf[\"n_estimators\"], max_features = hp_rf[\"max_features\"]\n",
    "                           , max_depth = hp_rf[\"max_depth\"])\n",
    "\n",
    "# fit the model on the training data\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "# use the model to predict the test set\n",
    "rf_y_pred = rf.predict(X_test)\n",
    "acc_mse = mean_squared_error(rf_y_pred,y_test)\n",
    "\n",
    "print(acc_mse)\n",
    "\n",
    "array_of_accuracies[\"Random Forest\"] = (acc_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic RF model better than linear regression\n",
    "K = 10\n",
    "\n",
    "# specifying list of parameters to be searched over\n",
    "lr = np.linspace(0.01,1,10)\n",
    "estimators = np.linspace(10,400,5)\n",
    "alg = [\"SAMME\",\"SAMME.R\"]\n",
    "\n",
    "# creating variables that will keep track of best score and parameters\n",
    "best_score = 10\n",
    "hp = {\"learning_rate\" : 0.01, \"algorithm\": \"SAMME\", \"n_estimators\": int(estimators[0])}\n",
    "counter = 1\n",
    "\n",
    "# looping over said parameters\n",
    "for j in lr:\n",
    "    for l in estimators:\n",
    "\n",
    "        # creating rf with the parameters and then getting the cross val score for it\n",
    "        ab = AdaBoostRegressor(random_state=123, n_estimators = int(l),learning_rate = j)\n",
    "        score = cross_val_score(ab,X_train,y_train,cv = K, scoring = \"neg_mean_squared_error\")\n",
    "\n",
    "        # storing the param if they show improvements\n",
    "        if(abs(np.mean(score)) < best_score):\n",
    "            best_score = abs(np.mean(score))\n",
    "            hp = {\"learning_rate\" : j, \"n_estimators\": int(l)}\n",
    "        counter+=1  \n",
    "        \n",
    "k = 10\n",
    "ab = AdaBoostRegressor(random_state=123)\n",
    "score = cross_val_score(ab,X_train,y_train,cv = k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'learning_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-cb97f0b8b410>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Creating hp_ab based on the best hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdaBoostRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp_ab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_estimators\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp_ab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# fitting the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'learning_rate'"
     ]
    }
   ],
   "source": [
    "# Assigning the best hyperparameters to hp_ab\n",
    "hp_ab = hp\n",
    "\n",
    "# Creating hp_ab based on the best hyperparameters\n",
    "ab = AdaBoostRegressor(random_state=123, n_estimators = hp_ab[\"n_estimators\"],learning_rate = hp_ab[\"learning_rate\"])\n",
    "\n",
    "# fitting the model\n",
    "ab.fit(X_train,y_train)\n",
    "\n",
    "# using the model to make prediction on the test set\n",
    "ab_y_pred = ab.predict(X_test)\n",
    "\n",
    "acc_mse = mean_squared_error(ab_y_pred,y_test)\n",
    "print(acc_mse)\n",
    "\n",
    "array_of_accuracies[\"AdaBoost\"] = (acc_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Linear Regression': 0.38434155015769855, 'Random Forest': 0.8050665935608865, 'AdaBoost': 0.40046531447686606}\n"
     ]
    }
   ],
   "source": [
    "print(array_of_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighboors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(n_neighbors=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = 0\n",
    "training_error = np.zeros(25)\n",
    "\n",
    "for i in range(1,26):\n",
    "    nn = KNeighborsRegressor(n_neighbors=i)\n",
    "    nn.fit(X_train, y_train)\n",
    "    training_error[counter] = (mean_squared_error(nn.predict(X_train),y_train))\n",
    "    counter += 1\n",
    "    \n",
    "x = np.linspace(1,25,25)\n",
    "a = sns.scatterplot(x = x,y = training_error)\n",
    "a.set(xlabel = 'number of neighbors', ylabel = 'training error')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "nn = KNeighborsRegressor(n_neighbors = 1)\n",
    "nn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the model\n",
    "y_test_nn = nn.predict(X_test)\n",
    "\n",
    "acc_mse = mean_squared_error(y_test_nn,y_test)\n",
    "\n",
    "array_of_accuracies[\"KNN\"] = acc_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7145199022095838\n"
     ]
    }
   ],
   "source": [
    "regr = MLPRegressor(random_state=1, learning_rate_init = 0.01, max_iter=500).fit(X_train, y_train)\n",
    "\n",
    "y_pred_nn = regr.predict(X_test)\n",
    "\n",
    "acc_mse = mean_squared_error(y_pred_nn,y_test)\n",
    "\n",
    "print(acc_mse)\n",
    "\n",
    "array_of_accuracies[\"Neural Network\"] = acc_mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Plot of all the Accuracies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['KNN'])\n",
      "[1.3447636457756884]\n"
     ]
    }
   ],
   "source": [
    "print(array_of_accuracies.keys())\n",
    "print(list(array_of_accuracies.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (9,8))\n",
    "plt.ylabel('Mean Squred Error')\n",
    "sns.set_context(\"paper\", font_scale=1)\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_style({'font.family': 'Times New Roman'})\n",
    "sns.barplot(x = list(array_of_accuracies.keys()), y = list(array_of_accuracies.values()))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing the Binary Task for Comparison with the Michigan Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_ratings = 1*(ratings['snippet rating'].values > 5)\n",
    "y = binary_ratings\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, binary_ratings, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression, Random Forests, AdaBoost, SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest \t0.95\n",
      "AdaBoost \t0.85\n",
      "Logistic R. \t0.85\n",
      "Naive Bayes \t0.85\n",
      "Bernoulli NB \t0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andoliveira/miniconda3/envs/re3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "supervised_model_classes = {\n",
    "                            \"Random Forest\": RandomForestClassifier,\n",
    "                            \"AdaBoost\": AdaBoostClassifier,\n",
    "                            \"Logistic R.\": LogisticRegression,\n",
    "                            \"Naive Bayes\": GaussianNB,\n",
    "                            \"Bernoulli NB\": BernoulliNB\n",
    "                            } \n",
    "\n",
    "for model_name, model_class in supervised_model_classes.items():\n",
    "    fit_model = model_class().fit(X_train, y_train)\n",
    "    y_pred = fit_model.predict(X_test)\n",
    "    l1 = accuracy_score(y_pred, y_test)\n",
    "    print(f\"{model_name} \\t{l1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
